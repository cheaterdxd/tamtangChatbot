import chainlit as cl
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_ollama import ChatOllama
# sá»­ dá»¥ng langchain_core
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- Cáº¤U HÃŒNH ---
DB_PATH = "./chroma_db"  # ÄÆ°á»ng dáº«n tá»›i folder DB báº¡n vá»«a táº¡o
EMBEDDING_MODEL = "intfloat/multilingual-e5-large-instruct" # Pháº£i khá»›p vá»›i model lÃºc Ingest
LLM_MODEL = "qwen2.5:7b" # Model cháº¡y trÃªn Ollama


llm = ChatOllama(
    model=LLM_MODEL,
    temperature=0.3, # Giá»¯ cho cÃ¢u tráº£ lá»i trang nghiÃªm, Ã­t sÃ¡ng táº¡o linh tinh
    base_url="http://localhost:11434"
)

def extract_filter(user_query, llm):
    """
    HÃ m nÃ y dÃ¹ng LLM Ä‘á»ƒ trÃ­ch xuáº¥t tÃªn kinh tá»« cÃ¢u há»i.
    Tráº£ vá»: TÃªn kinh chÃ­nh xÃ¡c hoáº·c "None"
    """
    router_template = """
    Báº¡n lÃ  má»™t há»‡ thá»‘ng phÃ¢n loáº¡i dá»¯ liá»‡u. 
    DÆ°á»›i Ä‘Ã¢y lÃ  danh sÃ¡ch cÃ¡c bá»™ Kinh cÃ³ trong cÆ¡ sá»Ÿ dá»¯ liá»‡u:
    {list_kinh}
    
    CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: "{query}"
    
    Nhiá»‡m vá»¥: 
    HÃ£y xem trong cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng cÃ³ nháº¯c Ä‘áº¿n tÃªn bá»™ Kinh nÃ o trong danh sÃ¡ch trÃªn khÃ´ng?
    - Náº¿u cÃ³: HÃ£y tráº£ vá» tÃªn chÃ­nh xÃ¡c cá»§a bá»™ Kinh Ä‘Ã³ (copy y há»‡t tá»« danh sÃ¡ch).
    - Náº¿u khÃ´ng: HÃ£y tráº£ vá» Ä‘Ãºng chá»¯ "None".
    
    Chá»‰ tráº£ vá» káº¿t quáº£, khÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm.
    """
    
    prompt = ChatPromptTemplate.from_template(router_template)
    chain = prompt | llm | StrOutputParser()
    
    # Cháº¡y chain Ä‘á»ƒ láº¥y tÃªn kinh
    extracted_kinh = chain.invoke({
        "list_kinh": "\n".join(LIST_KINH), 
        "query": user_query
    })
    
    return extracted_kinh.strip()

# 1. Khá»Ÿi táº¡o tÃ i nguyÃªn (Cháº¡y 1 láº§n khi app start)
@cl.on_chat_start
async def on_chat_start():
    msg = cl.Message(content="ğŸ™ Äang khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng Chatbot Pháº­t há»c...")
    await msg.send()

    # A. Load Embedding Model (Cháº¡y trÃªn GPU 1050Ti)
    embedding = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda'}, # DÃ¹ng GPU Ä‘á»ƒ vector hÃ³a cÃ¢u há»i user cho nhanh
        encode_kwargs={'normalize_embeddings': True}
    )

    # B. Load ChromaDB
    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embedding
    )
    
    # C. Táº¡o Retriever (NgÆ°á»i tÃ¬m kiáº¿m)
    # k=3: Láº¥y 3 Ä‘oáº¡n kinh vÄƒn liÃªn quan nháº¥t
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3} 
    )

    # # D. Load LLM (Cháº¡y trÃªn CPU Xeon - qua Ollama)
    # llm = ChatOllama(
    #     model=LLM_MODEL,
    #     temperature=0.3, # Giá»¯ cho cÃ¢u tráº£ lá»i trang nghiÃªm, Ã­t sÃ¡ng táº¡o linh tinh
    #     base_url="http://localhost:11434"
    # )

    # E. Thiáº¿t káº¿ Prompt (CÃ¢u tháº§n chÃº)
    template = """Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o Pháº­t há»c uy tÃ­n, thÃ´ng tuá»‡ kinh Ä‘iá»ƒn.
    HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong pháº§n Context bÃªn dÆ°á»›i.

    YÃªu cáº§u báº¯t buá»™c:
    1. Tráº£ lá»i báº±ng ngÃ´n ngá»¯ trang trá»ng, tá»« bi, Ä‘Ãºng chÃ¡nh phÃ¡p.
    2. Náº¿u trong Context cÃ³ Ä‘oáº¡n tiáº¿ng Pali, hÃ£y trÃ­ch dáº«n nguyÃªn vÄƒn cÃ¢u Pali Ä‘Ã³ ra.
    3. Cuá»‘i cÃ¢u tráº£ lá»i, hÃ£y ghi rÃµ nguá»“n trÃ­ch dáº«n (TÃªn Kinh, TÃªn Pháº©m) cÃ³ trong Context.
    4. Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong Context, hÃ£y nÃ³i "Táº¡i háº¡ chÆ°a tÃ¬m tháº¥y thÃ´ng tin nÃ y trong kho dá»¯ liá»‡u hiá»‡n táº¡i", tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘Æ°á»£c tá»± bá»‹a ra.

    Context (Kinh vÄƒn tham kháº£o):
    {context}

    CÃ¢u há»i cá»§a thÃ­ chá»§: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # F. Táº¡o Chain (DÃ¢y chuyá»n xá»­ lÃ½)
    def format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs])

    # LÆ°u retriever vÃ  runable vÃ o session user Ä‘á»ƒ dÃ¹ng láº¡i
    cl.user_session.set("retriever", retriever)
    
    # Äá»‹nh nghÄ©a luá»“ng RAG cÆ¡ báº£n
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    cl.user_session.set("rag_chain", rag_chain)
    
    msg.content = f"ğŸª· Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng! Äang sá»­ dá»¥ng trÃ­ tuá»‡ cá»§a {LLM_MODEL}."
    await msg.update()

# 2. Xá»­ lÃ½ khi ngÆ°á»i dÃ¹ng chat
@cl.on_message
async def on_message(message: cl.Message):
    rag_chain = cl.user_session.get("rag_chain")
    retriever = cl.user_session.get("retriever")

    detected_kinh = await cl.make_async(extract_filter)(message.content, llm)
    search_kwargs = {"k": 3}
    if detected_kinh != "None" and detected_kinh in LIST_KINH:
        print(f"ğŸ¯ Äang lá»c theo: {detected_kinh}") # Log Ä‘á»ƒ debug
        search_kwargs["filter"] = {"Ten_Kinh": detected_kinh}
        
        # Gá»­i thÃ´ng bÃ¡o nhá» cho user biáº¿t (Optional)
        await cl.Message(content=f"ğŸ” Äang tÃ¬m kiáº¿m giá»›i háº¡n trong: **{detected_kinh}**").send()
    else:
        print("ğŸŒ Äang tÃ¬m trÃªn toÃ n bá»™ dá»¯ liá»‡u")
    # BÆ°á»›c 1: TÃ¬m kiáº¿m tÃ i liá»‡u nguá»“n (Ä‘á»ƒ hiá»ƒn thá»‹ cho user xem)
    source_documents = await cl.make_async(retriever.invoke)(message.content)
    
    # Táº¡o cÃ¡c element hiá»ƒn thá»‹ nguá»“n (Text Box Ä‘áº¹p máº¯t)
    text_elements = []
    if source_documents:
        for i, doc in enumerate(source_documents):
            source_name = f"Nguá»“n {i+1}: {doc.metadata.get('Ten_Kinh', 'N/A')} - {doc.metadata.get('Ten_Pham', 'N/A')}"
            text_elements.append(
                cl.Text(content=doc.page_content, name=source_name, display="side")
            )

    # BÆ°á»›c 2: Gá»­i cÃ¢u há»i cho LLM vÃ  Stream cÃ¢u tráº£ lá»i vá»
    msg = cl.Message(content="", elements=text_elements)
    
    async for chunk in rag_chain.astream(message.content):
        await msg.stream_token(chunk)

    await msg.send()