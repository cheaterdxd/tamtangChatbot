import chainlit as cl
from langchain_ollama import ChatOllama
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import os, difflib, re

# ==================================================
# 1. Cáº¤U HÃŒNH & Xá»¬ LÃ Dá»® LIá»†U LIST.MD (QUAN TRá»ŒNG)
# ==================================================

def load_and_parse_list_kinh(file_path):
    """
    Äá»c file list.md vÃ  tÃ¡ch ra 2 danh sÃ¡ch:
    1. full_lines: Äá»ƒ gá»­i cho LLM Ä‘á»c hiá»ƒu ngá»¯ cáº£nh.
    2. clean_metadata_keys: Chá»‰ chá»©a pháº§n Header 3 (cá»™t cuá»‘i cÃ¹ng) Ä‘á»ƒ filter DB.
    """
    full_lines = []
    clean_metadata_keys = []
    
    try:
        if not os.path.exists(file_path):
            print(f"âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y file {file_path}")
            return [], []
            
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"): continue # Bá» qua dÃ²ng trá»‘ng hoáº·c header
                
                full_lines.append(line)
                
                # Logic Parse: TÃ¡ch theo dáº¥u '|' vÃ  láº¥y pháº§n tá»­ cuá»‘i cÃ¹ng
                # VÃ¬ cáº¥u trÃºc file cá»§a báº¡n lÃ : No. Name | Header 1 | Header 2 | Header 3 (Metadata Target)
                parts = line.split("|")
                if len(parts) > 1:
                    # Láº¥y pháº§n cuá»‘i cÃ¹ng vÃ  xÃ³a khoáº£ng tráº¯ng thá»«a
                    target_key = parts[-1].strip()
                    clean_metadata_keys.append(target_key)
                else:
                    # Fallback náº¿u dÃ²ng khÃ´ng cÃ³ dáº¥u |
                    clean_metadata_keys.append(line)
                    
        return full_lines, clean_metadata_keys
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file list.md: {e}")
        return [], []
# Load dá»¯ liá»‡u
LIST_FILE_PATH = "data/Truong_Bo_Kinh_Final/list.md"
LIST_KINH_FULL, LIST_METADATA_CLEAN = load_and_parse_list_kinh(LIST_FILE_PATH)

print(f"DEBUG: ÄÃ£ load {len(LIST_METADATA_CLEAN)} kinh target.")

if LIST_METADATA_CLEAN:
    print(f"DEBUG Sample Target: '{LIST_METADATA_CLEAN[1]}'") # NÃªn ra: BrahmajÄlasuttaá¹ƒ(Kinh Pháº¡m VÃµng)

def normalize_kinh_name(llm_output, metadata_list):
    """
    So khá»›p output cá»§a LLM vá»›i danh sÃ¡ch Metadata CHUáº¨N (cá»™t cuá»‘i cÃ¹ng).
    """
    if not llm_output or "None" in llm_output:
        return None
        
    # 1. Vá»‡ sinh Output cá»§a LLM (XÃ³a sá»‘ thá»© tá»± kiá»ƒu "1. ", "07. ")
    # LLM thÆ°á»ng tráº£ vá»: "1. BrahmajÄlasuttaá¹ƒ" -> cáº§n clean thÃ nh "BrahmajÄlasuttaá¹ƒ"
    clean_output = re.sub(r'^\d+[\.\s]+', '', llm_output).strip()
    
    # 2. TÃ¬m kiáº¿m chÃ­nh xÃ¡c trÆ°á»›c (Case insensitive)
    for key in metadata_list:
        if clean_output.lower() == key.lower():
            return key
            
    # 3. TÃ¬m kiáº¿m gáº§n Ä‘Ãºng (Fuzzy Match)
    # cutoff=0.4: Cháº¥p nháº­n Ä‘á»™ giá»‘ng 40% (vÃ¬ output LLM thÆ°á»ng ngáº¯n hÆ¡n tÃªn Ä‘áº§y Ä‘á»§ trong DB)
    matches = difflib.get_close_matches(clean_output, metadata_list, n=1, cutoff=0.4)
    
    if matches:
        return matches[0]
    
    # 4. Fallback: Kiá»ƒm tra "chá»©a trong" (Contains)
    # VÃ­ dá»¥ LLM ra "Pháº¡m VÃµng" -> Match vá»›i "BrahmajÄlasuttaá¹ƒ(Kinh Pháº¡m VÃµng)"
    for key in metadata_list:
        if clean_output.lower() in key.lower():
            return key
            
    return None

# Cáº¥u hÃ¬nh Local LLM
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "qwen2.5:7b"
EMBEDDING_MODEL = "intfloat/multilingual-e5-large-instruct"
DB_VECTOR="./chroma_db3"
# ==================================================
# 2. HÃ€M ROUTER (TRÃCH XUáº¤T FILTER)
# ==================================================
def extract_filter(query: str, llm):
    """
    HÃ m nÃ y há»i LLM xem cÃ¢u há»i thuá»™c vá» bá»™ kinh nÃ o trong LIST_KINH.
    """
    router_template = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ phÃ¢n loáº¡i tÃ i liá»‡u Pháº­t giÃ¡o.
    Nhiá»‡m vá»¥: XÃ¡c Ä‘á»‹nh xem cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng Ä‘ang Ä‘á» cáº­p Ä‘áº¿n bá»™ kinh nÃ o trong danh sÃ¡ch dÆ°á»›i Ä‘Ã¢y.
    
    DANH SÃCH KINH:
    {list_kinh}
    
    CÃ‚U Há»ŽI: {question}
    
    YÃŠU Cáº¦U:
    - Náº¿u cÃ¢u há»i nháº¯c Ä‘áº¿n tÃªn kinh hoáº·c ná»™i dung Ä‘áº·c thÃ¹ cá»§a má»™t kinh trong danh sÃ¡ch, hÃ£y tráº£ vá» CHÃNH XÃC tÃªn Ä‘Ã³.
    - Náº¿u khÃ´ng xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c hoáº·c cÃ¢u há»i chung chung, hÃ£y tráº£ vá» "None".
    - CHá»ˆ TRáº¢ Vá»€ TÃŠN KINH HOáº¶C "None". KHÃ”NG GIáº¢I THÃCH.
    """
    
    prompt = ChatPromptTemplate.from_template(router_template)
    chain = prompt | llm | StrOutputParser()
    
    try:
        # Gá»­i FULL LIST cho LLM Ä‘á»ƒ nÃ³ cÃ³ ngá»¯ cáº£nh (bao gá»“m cáº£ tÃªn Pali vÃ  Viá»‡t á»Ÿ cá»™t Ä‘áº§u)
        result = chain.invoke({"list_kinh": "\n".join(LIST_KINH_FULL), "question": query})
        cleaned_result = result.strip().replace("'", "").replace('"', "")
        return cleaned_result
    except Exception as e:
        print(f"Lá»—i Router: {e}")
        return "None"

# ==================================================
# 3. KHá»žI Táº O SESSION (ON START)
# ==================================================
@cl.on_chat_start
async def on_chat_start():
    print("--- Báº¯t Ä‘áº§u khá»Ÿi táº¡o App ---")
    
    # A. Load Embeddings 
    model_kwargs = {'device': 'cpu'} # Hoáº·c cuda
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
        multi_process=False
    )
    
    # B. Load VectorStore (Chroma)
    # Giáº£ sá»­ báº¡n lÆ°u DB á»Ÿ thÆ° má»¥c "./chroma_db"
    if not os.path.exists(DB_VECTOR):
         await cl.Message("âš ï¸ Lá»—i: KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c './chroma_db2'. Vui lÃ²ng Ingest dá»¯ liá»‡u trÆ°á»›c!").send()
         return

    vectorstore = Chroma(
        persist_directory=DB_VECTOR,
        embedding_function=embeddings # Äá»•i tÃªn collection náº¿u báº¡n Ä‘áº·t khÃ¡c
    )
    
    # C. Load LLM (Ollama)
    llm = ChatOllama(
        base_url=OLLAMA_URL,
        model=MODEL_NAME,
        temperature=0.2 # Router cáº§n chÃ­nh xÃ¡c, temperature tháº¥p
    )
    
    # D. LÆ°u vÃ o Session Ä‘á»ƒ dÃ¹ng láº¡i á»Ÿ má»—i tin nháº¯n
    cl.user_session.set("vectorstore", vectorstore)
    cl.user_session.set("llm", llm)
    
    await cl.Message(content="Trá»£ lÃ½ áº£o Ä‘Ã£ sáºµn sÃ ng!").send()

# ==================================================
# 4. Xá»¬ LÃ TIN NHáº®N (MAIN LOGIC)
# ==================================================
@cl.on_message
async def on_message(message: cl.Message):
    llm = cl.user_session.get("llm")
    vectorstore = cl.user_session.get("vectorstore")
    
    user_query = f"Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: {message.content}"
    
    msg_processing = cl.Message(content="ðŸ¤” Äang suy nghÄ©...")
    await msg_processing.send()
    
    # 1. Router cháº¡y
    detected_kinh_raw = await cl.make_async(extract_filter)(user_query, llm)
    print(f"ðŸ¤– Router Output: {detected_kinh_raw}") # VD: 1. BrahmajÄlasuttaá¹ƒ
    
    # 2. Chuáº©n hÃ³a: Map output cá»§a LLM vÃ o LIST_METADATA_CLEAN
    detected_kinh = normalize_kinh_name(detected_kinh_raw, LIST_METADATA_CLEAN)
    print(f"ðŸŽ¯ DB Key Normalized: {detected_kinh}")

    search_kwargs = {"k": 3}
    
    if detected_kinh:
        # QUAN TRá»ŒNG: LÃºc nÃ y detected_kinh Ä‘Ã£ khá»›p 100% vá»›i DB Metadata
        search_kwargs["filter"] = {
            "$or": [    
                {"Ten_bai_kinh": detected_kinh},
                {"Ten_bo_kinh": detected_kinh}
            ]
        }
        filter_msg = f"\n*(Giá»›i háº¡n tÃ¬m kiáº¿m: **{detected_kinh}**)*"
    else:
        # Náº¿u Router ra None hoáº·c map khÃ´ng Ä‘Æ°á»£c -> TÃ¬m táº¥t cáº£
        print("ðŸŒ Searching all documents (Fallback)")
        filter_msg = "\n*(TÃ¬m kiáº¿m trÃªn toÃ n bá»™ dá»¯ liá»‡u)*"

    print(f"DEBUG FILTER KWARGS: {search_kwargs}") # Kiá»ƒm tra láº§n cuá»‘i á»Ÿ Ä‘Ã¢y

    # 3. Retrieve
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs=search_kwargs
    )
    
    docs = await retriever.ainvoke(user_query)
    
    # Debug in ra terminal (náº¿u muá»‘n)
    print(f"DEBUG: TÃ¬m tháº¥y {len(docs)} docs")

    # Táº¡o cÃ¡c pháº§n tá»­ UI (Sources) Ä‘á»ƒ hiá»ƒn thá»‹ trÃªn Chainlit
    source_elements = []
    for i, doc in enumerate(docs):
        # Láº¥y tÃªn nguá»“n tá»« metadata
        source_name = f"Nguá»“n {i+1} ({doc.metadata.get('Ten_bai_kinh', 'Doc')})"
        
        # Táº¡o text element
        source_elements.append(
            cl.Text(content=doc.page_content, name=source_name, display="side")
        )
    
    if not docs:
        await msg_processing.remove()
        await cl.Message(content=filter_msg + "\n\nXin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong kinh Ä‘iá»ƒn Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y.").send()
        return

# --- BÆ¯á»šC 5: GENERATION (CHá»ˆ CÃ’N LLM) ---
    
    rag_template = """
    Dá»±a vÃ o cÃ¡c Ä‘oáº¡n vÄƒn sau Ä‘Ã¢y tá»« kinh Ä‘iá»ƒn Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    Náº¿u khÃ´ng cÃ³ thÃ´ng tin, hÃ£y nÃ³i lÃ  khÃ´ng biáº¿t, Ä‘á»«ng bá»‹a ra.
    
    Context:
    {context}
    
    CÃ¢u há»i: {question}
    
    YÃªu cáº§u: Tráº£ lá»i (chi tiáº¿t vÃ  trang nghiÃªm) theo cÃ¡c thÃ´ng tin Ä‘Ã£ cÃ³.
    NgÃ´n ngá»¯: chá»‰ sá»­ dá»¥ng ngÃ´n ngá»¯ tiáº¿ng Viá»‡t. 
    """
    rag_prompt = ChatPromptTemplate.from_template(rag_template)
    
    # Format docs thÃ nh chuá»—i string
    context_str = "\n\n".join(doc.page_content for doc in docs)
    
    # Äá»‹nh nghÄ©a Chain (LÃºc nÃ y chá»‰ cÃ²n Prompt -> LLM -> Parser)
    # VÃ¬ ta Ä‘Ã£ cÃ³ context_str rá»“i, khÃ´ng cáº§n retriever trong chain ná»¯a
    runnable = rag_prompt | llm | StrOutputParser()
    
    # --- BÆ¯á»šC 6: STREAM Káº¾T QUáº¢ ---
    # Gá»­i message cÃ³ Ä‘Ã­nh kÃ¨m source_elements
    res = cl.Message(content=filter_msg + "\n\n", elements=source_elements)
    
    # Cháº¡y chain vá»›i input trá»±c tiáº¿p
    async for chunk in runnable.astream({"context": context_str, "question": user_query}):
        await res.stream_token(chunk)
    
    await res.send()
    await msg_processing.remove()