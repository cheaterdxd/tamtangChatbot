import asyncio
import re
import os
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup

# --- C·∫§U H√åNH ---
# Link danh m·ª•c Kinh Trung B·ªô
BASE_URL = "https://tipitaka.com.vn/category/tipi%e1%b9%adaka/tipi%e1%b9%adaka-mula/suttapitaka-tang-kinh/majjhimanikaya-kinh-trung-bo/"
OUTPUT_FOLDER = "data/Kinh_Trung_Bo_Data"
START_PAGE = 1
# Kinh Trung B·ªô c√≥ 152 b√†i, ta c·ª© cho ch·∫°y d∆∞ ra 1 ch√∫t, code s·∫Ω t·ª± d·ª´ng n·∫øu h·∫øt b√†i
MAX_PAGES = 160 

# T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ
if not os.path.exists(OUTPUT_FOLDER):
    os.makedirs(OUTPUT_FOLDER)

def clean_text(text):
    if not text: return ""
    # X√≥a c√°c k√Ω t·ª± xu·ªëng d√≤ng th·ª´a, kho·∫£ng tr·∫Øng k√©p
    text = text.replace('\xa0', ' ').replace('\n', ' ').replace('\r', '').strip()
    return re.sub(r'\s+', ' ', text)

def sanitize_filename(name):
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng ƒë∆∞·ª£c ph√©p trong t√™n file Windows/Mac
    safe_name = re.sub(r'[\\/*?:"<>|]', "", name)
    return safe_name.strip()

async def crawl_page(crawler, page_num):
    # T·∫°o URL: Trang 1 l√† base url, c√°c trang sau th√™m /page/x/
    if page_num == 1:
        url = BASE_URL
    else:
        url = f"{BASE_URL}page/{page_num}/"

    print(f"‚è≥ ƒêang x·ª≠ l√Ω Trang {page_num}: {url}")

    # Crawl v√† ch·ªù th·∫ª div.entry-content xu·∫•t hi·ªán
    result = await crawler.arun(
        url=url,
        bypass_cache=True,
        wait_for="div.entry-content"
    )

    if not result.success:
        print(f"   ‚ùå Kh√¥ng th·ªÉ truy c·∫≠p trang {page_num} (C√≥ th·ªÉ ƒë√£ h·∫øt b√†i).")
        return False # B√°o hi·ªáu d·ª´ng l·∫°i

    soup = BeautifulSoup(result.html, 'html.parser')
    entry_content = soup.select_one("div.entry-content")

    if not entry_content:
        print(f"   ‚ö†Ô∏è Trang {page_num} kh√¥ng c√≥ n·ªôi dung b√†i kinh.")
        return False

    # L·∫•y t·∫•t c·∫£ c√°c d√≤ng trong b·∫£ng
    rows = entry_content.select("table tr")
    md_output = []
    
    # Bi·∫øn l∆∞u t√™n file
    filename_title = f"Bai_kinh_so_{page_num}"
    found_title = False

    for row in rows:
        cols = row.find_all("td")
        if len(cols) >= 2:
            pali = clean_text(cols[0].get_text())
            viet = clean_text(cols[1].get_text())

            # B·ªè qua c√°c d√≤ng r√°c h·ªá th·ªëng
            if (not pali and not viet) or "Tipitaka.org" in pali or "Vi·ªát d·ªãch" in viet:
                continue

            # --- LOGIC T√åM T√äN B√ÄI KINH ƒê·ªÇ ƒê·∫∂T T√äN FILE ---
            # Th∆∞·ªùng d√≤ng ti√™u ƒë·ªÅ s·∫Ω ch·ª©a t·ª´ "sutta" (kinh) v√† ng·∫Øn g·ªçn
            if not found_title:
                if len(pali) < 150 and ("sutta" in pali.lower() or "kinh" in viet.lower()):
                    # T·∫°o t√™n file: VD "01. M≈´lapariyƒÅyasutta·πÉ - Kinh ph√°p m√¥n cƒÉn b·∫£n"
                    raw_title = f"{pali} - {viet}"
                    filename_title = sanitize_filename(raw_title)
                    
                    # ƒê∆∞a ti√™u ƒë·ªÅ v√†o n·ªôi dung file lu√¥n
                    md_output.append(f"# {raw_title}\n")
                    found_title = True
                    continue 

            # --- N·ªòI DUNG ---
            # Format: [Pali] \n [Vi·ªát]
            md_output.append(f"[{pali}]\n[{viet}]\n")

    # --- L∆ØU FILE ---
    if md_output:
        # N·∫øu kh√¥ng t√¨m th·∫•y ti√™u ƒë·ªÅ trong b·∫£ng, ta d√πng s·ªë trang l√†m t√™n
        file_path = os.path.join(OUTPUT_FOLDER, f"{filename_title}.md")
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"Source: {url}\n\n")
            f.write("\n".join(md_output))
        
        print(f"   ‚úÖ ƒê√£ l∆∞u: {filename_title}.md")
        return True # Ti·∫øp t·ª•c trang sau
    else:
        print(f"   ‚ö†Ô∏è Trang {page_num} tr·ªëng d·ªØ li·ªáu b·∫£ng.")
        return False # D·ª´ng l·∫°i

async def main():
    async with AsyncWebCrawler(verbose=False) as crawler:
        print(f"üöÄ B·∫Øt ƒë·∫ßu crawl Kinh Trung B·ªô ({MAX_PAGES} trang d·ª± ki·∫øn)...")
        
        for i in range(START_PAGE, MAX_PAGES + 1):
            success = await crawl_page(crawler, i)
            
            # N·∫øu g·∫∑p l·ªói ho·∫∑c h·∫øt trang th√¨ d·ª´ng v√≤ng l·∫∑p
            if not success:
                print("üõë ƒê√£ h·∫øt b√†i ho·∫∑c g·∫∑p l·ªói. D·ª´ng ch∆∞∆°ng tr√¨nh.")
                break
                
            # Ngh·ªâ 1 gi√¢y ƒë·ªÉ server kh√¥ng ch·∫∑n
            await asyncio.sleep(1) 
            
        print("\nüéâ HO√ÄN T·∫§T! Ki·ªÉm tra th∆∞ m·ª•c 'Kinh_Trung_Bo_Data'")

if __name__ == "__main__":
    asyncio.run(main())